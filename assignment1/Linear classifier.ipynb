{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\пользователь\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "c:\\users\\пользователь\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int64)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, 2, size=(batch_size, 1)).astype(np.int64).reshape(1, -1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float64)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float64)\n",
    "target_index = np.ones(batch_size, dtype=np.int64)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1055.086909\n",
      "Epoch 1, loss: 870.183439\n",
      "Epoch 2, loss: 1037.316240\n",
      "Epoch 3, loss: 953.088692\n",
      "Epoch 4, loss: 776.617279\n",
      "Epoch 5, loss: 959.029192\n",
      "Epoch 6, loss: 977.682835\n",
      "Epoch 7, loss: 1060.178294\n",
      "Epoch 8, loss: 848.900040\n",
      "Epoch 9, loss: 824.920023\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ff1e4515c8>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.169\n",
      "Epoch 0, loss: 820.541176\n",
      "Epoch 1, loss: 919.686933\n",
      "Epoch 2, loss: 820.996239\n",
      "Epoch 3, loss: 920.991001\n",
      "Epoch 4, loss: 864.140768\n",
      "Epoch 5, loss: 1031.126230\n",
      "Epoch 6, loss: 1000.033425\n",
      "Epoch 7, loss: 895.569659\n",
      "Epoch 8, loss: 978.593129\n",
      "Epoch 9, loss: 859.282500\n",
      "Epoch 10, loss: 1062.394772\n",
      "Epoch 11, loss: 900.900964\n",
      "Epoch 12, loss: 856.455941\n",
      "Epoch 13, loss: 932.153493\n",
      "Epoch 14, loss: 888.408562\n",
      "Epoch 15, loss: 776.167241\n",
      "Epoch 16, loss: 1012.114524\n",
      "Epoch 17, loss: 872.038077\n",
      "Epoch 18, loss: 1125.417483\n",
      "Epoch 19, loss: 747.960765\n",
      "Epoch 20, loss: 955.321735\n",
      "Epoch 21, loss: 915.970179\n",
      "Epoch 22, loss: 846.705540\n",
      "Epoch 23, loss: 1022.794247\n",
      "Epoch 24, loss: 899.288083\n",
      "Epoch 25, loss: 925.442639\n",
      "Epoch 26, loss: 720.157794\n",
      "Epoch 27, loss: 887.710059\n",
      "Epoch 28, loss: 823.255647\n",
      "Epoch 29, loss: 870.011700\n",
      "Epoch 30, loss: 920.217649\n",
      "Epoch 31, loss: 859.589870\n",
      "Epoch 32, loss: 847.248802\n",
      "Epoch 33, loss: 903.310686\n",
      "Epoch 34, loss: 804.341868\n",
      "Epoch 35, loss: 1009.692522\n",
      "Epoch 36, loss: 1011.892955\n",
      "Epoch 37, loss: 786.128657\n",
      "Epoch 38, loss: 848.740156\n",
      "Epoch 39, loss: 897.395242\n",
      "Epoch 40, loss: 963.605261\n",
      "Epoch 41, loss: 984.129738\n",
      "Epoch 42, loss: 754.416038\n",
      "Epoch 43, loss: 917.123851\n",
      "Epoch 44, loss: 1036.087404\n",
      "Epoch 45, loss: 818.536320\n",
      "Epoch 46, loss: 910.725171\n",
      "Epoch 47, loss: 1071.122951\n",
      "Epoch 48, loss: 843.503056\n",
      "Epoch 49, loss: 809.361764\n",
      "Epoch 50, loss: 966.663487\n",
      "Epoch 51, loss: 908.500095\n",
      "Epoch 52, loss: 894.518199\n",
      "Epoch 53, loss: 749.106649\n",
      "Epoch 54, loss: 807.036676\n",
      "Epoch 55, loss: 1032.772603\n",
      "Epoch 56, loss: 995.398206\n",
      "Epoch 57, loss: 927.187547\n",
      "Epoch 58, loss: 922.646634\n",
      "Epoch 59, loss: 909.850314\n",
      "Epoch 60, loss: 768.386179\n",
      "Epoch 61, loss: 1038.151076\n",
      "Epoch 62, loss: 976.072670\n",
      "Epoch 63, loss: 1024.876928\n",
      "Epoch 64, loss: 921.393645\n",
      "Epoch 65, loss: 1077.562104\n",
      "Epoch 66, loss: 917.478319\n",
      "Epoch 67, loss: 894.405118\n",
      "Epoch 68, loss: 840.803822\n",
      "Epoch 69, loss: 831.135050\n",
      "Epoch 70, loss: 837.271429\n",
      "Epoch 71, loss: 922.797669\n",
      "Epoch 72, loss: 882.824683\n",
      "Epoch 73, loss: 810.634503\n",
      "Epoch 74, loss: 867.686926\n",
      "Epoch 75, loss: 906.523125\n",
      "Epoch 76, loss: 952.152220\n",
      "Epoch 77, loss: 811.639047\n",
      "Epoch 78, loss: 972.388102\n",
      "Epoch 79, loss: 1083.073508\n",
      "Epoch 80, loss: 852.591947\n",
      "Epoch 81, loss: 784.689578\n",
      "Epoch 82, loss: 762.820136\n",
      "Epoch 83, loss: 814.103684\n",
      "Epoch 84, loss: 933.708267\n",
      "Epoch 85, loss: 1006.696504\n",
      "Epoch 86, loss: 954.653277\n",
      "Epoch 87, loss: 807.434336\n",
      "Epoch 88, loss: 1022.632481\n",
      "Epoch 89, loss: 1001.687888\n",
      "Epoch 90, loss: 920.230751\n",
      "Epoch 91, loss: 983.690700\n",
      "Epoch 92, loss: 1051.929963\n",
      "Epoch 93, loss: 1024.445475\n",
      "Epoch 94, loss: 805.552001\n",
      "Epoch 95, loss: 828.450826\n",
      "Epoch 96, loss: 901.780715\n",
      "Epoch 97, loss: 755.090120\n",
      "Epoch 98, loss: 949.273135\n",
      "Epoch 99, loss: 877.625055\n",
      "Accuracy after training for 100 epochs:  0.167\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 864.123209\n",
      "Epoch 1, loss: 1031.151524\n",
      "Epoch 2, loss: 854.462479\n",
      "Epoch 3, loss: 745.929690\n",
      "Epoch 4, loss: 792.352896\n",
      "Epoch 5, loss: 989.778082\n",
      "Epoch 6, loss: 730.628475\n",
      "Epoch 7, loss: 956.406596\n",
      "Epoch 8, loss: 705.386016\n",
      "Epoch 9, loss: 745.762882\n",
      "Epoch 10, loss: 691.949937\n",
      "Epoch 11, loss: 804.413547\n",
      "Epoch 12, loss: 759.395383\n",
      "Epoch 13, loss: 851.785196\n",
      "Epoch 14, loss: 737.258366\n",
      "Epoch 15, loss: 742.892914\n",
      "Epoch 16, loss: 797.439111\n",
      "Epoch 17, loss: 794.944468\n",
      "Epoch 18, loss: 794.787950\n",
      "Epoch 19, loss: 806.885664\n",
      "Epoch 20, loss: 854.534535\n",
      "Epoch 21, loss: 750.034602\n",
      "Epoch 22, loss: 729.663051\n",
      "Epoch 23, loss: 797.610118\n",
      "Epoch 24, loss: 804.528812\n",
      "Epoch 25, loss: 874.416972\n",
      "Epoch 26, loss: 852.958736\n",
      "Epoch 27, loss: 805.065720\n",
      "Epoch 28, loss: 702.671076\n",
      "Epoch 29, loss: 744.252845\n",
      "Epoch 30, loss: 780.984563\n",
      "Epoch 31, loss: 1188.784439\n",
      "Epoch 32, loss: 860.750681\n",
      "Epoch 33, loss: 852.897230\n",
      "Epoch 34, loss: 721.506489\n",
      "Epoch 35, loss: 728.160632\n",
      "Epoch 36, loss: 909.473417\n",
      "Epoch 37, loss: 857.678355\n",
      "Epoch 38, loss: 813.143273\n",
      "Epoch 39, loss: 832.418296\n",
      "Epoch 40, loss: 900.091021\n",
      "Epoch 41, loss: 934.790357\n",
      "Epoch 42, loss: 822.443774\n",
      "Epoch 43, loss: 794.695701\n",
      "Epoch 44, loss: 761.279793\n",
      "Epoch 45, loss: 745.443393\n",
      "Epoch 46, loss: 807.136792\n",
      "Epoch 47, loss: 900.792041\n",
      "Epoch 48, loss: 729.962181\n",
      "Epoch 49, loss: 680.762361\n",
      "Epoch 50, loss: 791.161050\n",
      "Epoch 51, loss: 764.375952\n",
      "Epoch 52, loss: 687.503304\n",
      "Epoch 53, loss: 782.522318\n",
      "Epoch 54, loss: 668.901248\n",
      "Epoch 55, loss: 801.121596\n",
      "Epoch 56, loss: 881.576537\n",
      "Epoch 57, loss: 689.228257\n",
      "Epoch 58, loss: 695.577177\n",
      "Epoch 59, loss: 866.435445\n",
      "Epoch 60, loss: 1105.368734\n",
      "Epoch 61, loss: 694.403426\n",
      "Epoch 62, loss: 745.131361\n",
      "Epoch 63, loss: 728.482754\n",
      "Epoch 64, loss: 693.430475\n",
      "Epoch 65, loss: 734.119408\n",
      "Epoch 66, loss: 760.940422\n",
      "Epoch 67, loss: 717.535105\n",
      "Epoch 68, loss: 689.117711\n",
      "Epoch 69, loss: 627.556576\n",
      "Epoch 70, loss: 664.978964\n",
      "Epoch 71, loss: 810.640542\n",
      "Epoch 72, loss: 764.490107\n",
      "Epoch 73, loss: 812.951542\n",
      "Epoch 74, loss: 774.570947\n",
      "Epoch 75, loss: 715.016367\n",
      "Epoch 76, loss: 820.099012\n",
      "Epoch 77, loss: 731.357103\n",
      "Epoch 78, loss: 781.700405\n",
      "Epoch 79, loss: 854.776774\n",
      "Epoch 80, loss: 656.510834\n",
      "Epoch 81, loss: 669.355171\n",
      "Epoch 82, loss: 840.130308\n",
      "Epoch 83, loss: 705.895207\n",
      "Epoch 84, loss: 786.649297\n",
      "Epoch 85, loss: 788.274322\n",
      "Epoch 86, loss: 729.473136\n",
      "Epoch 87, loss: 820.268884\n",
      "Epoch 88, loss: 797.397156\n",
      "Epoch 89, loss: 857.327268\n",
      "Epoch 90, loss: 700.252296\n",
      "Epoch 91, loss: 805.932069\n",
      "Epoch 92, loss: 675.239692\n",
      "Epoch 93, loss: 676.140598\n",
      "Epoch 94, loss: 743.208140\n",
      "Epoch 95, loss: 697.322085\n",
      "Epoch 96, loss: 810.914307\n",
      "Epoch 97, loss: 667.106105\n",
      "Epoch 98, loss: 722.014391\n",
      "Epoch 99, loss: 781.228824\n",
      "Epoch 100, loss: 813.630303\n",
      "Epoch 101, loss: 719.846629\n",
      "Epoch 102, loss: 862.849423\n",
      "Epoch 103, loss: 750.968258\n",
      "Epoch 104, loss: 666.507572\n",
      "Epoch 105, loss: 782.109116\n",
      "Epoch 106, loss: 695.888627\n",
      "Epoch 107, loss: 674.129006\n",
      "Epoch 108, loss: 834.771249\n",
      "Epoch 109, loss: 663.426917\n",
      "Epoch 110, loss: 627.745636\n",
      "Epoch 111, loss: 664.582414\n",
      "Epoch 112, loss: 787.148274\n",
      "Epoch 113, loss: 759.244112\n",
      "Epoch 114, loss: 663.303764\n",
      "Epoch 115, loss: 764.693780\n",
      "Epoch 116, loss: 781.882013\n",
      "Epoch 117, loss: 801.100832\n",
      "Epoch 118, loss: 769.695789\n",
      "Epoch 119, loss: 670.610369\n",
      "Epoch 120, loss: 672.816848\n",
      "Epoch 121, loss: 700.820664\n",
      "Epoch 122, loss: 924.714235\n",
      "Epoch 123, loss: 688.038201\n",
      "Epoch 124, loss: 625.976942\n",
      "Epoch 125, loss: 804.134969\n",
      "Epoch 126, loss: 908.081040\n",
      "Epoch 127, loss: 773.139652\n",
      "Epoch 128, loss: 795.932514\n",
      "Epoch 129, loss: 709.199646\n",
      "Epoch 130, loss: 680.220631\n",
      "Epoch 131, loss: 793.935413\n",
      "Epoch 132, loss: 697.166652\n",
      "Epoch 133, loss: 707.547750\n",
      "Epoch 134, loss: 733.991896\n",
      "Epoch 135, loss: 703.906542\n",
      "Epoch 136, loss: 637.483988\n",
      "Epoch 137, loss: 753.900826\n",
      "Epoch 138, loss: 798.305140\n",
      "Epoch 139, loss: 892.099816\n",
      "Epoch 140, loss: 913.402434\n",
      "Epoch 141, loss: 649.725616\n",
      "Epoch 142, loss: 717.855859\n",
      "Epoch 143, loss: 782.036144\n",
      "Epoch 144, loss: 693.588806\n",
      "Epoch 145, loss: 892.734953\n",
      "Epoch 146, loss: 635.869297\n",
      "Epoch 147, loss: 630.620341\n",
      "Epoch 148, loss: 681.248140\n",
      "Epoch 149, loss: 746.085819\n",
      "Epoch 150, loss: 782.682707\n",
      "Epoch 151, loss: 753.450517\n",
      "Epoch 152, loss: 755.078968\n",
      "Epoch 153, loss: 707.726559\n",
      "Epoch 154, loss: 758.612820\n",
      "Epoch 155, loss: 751.653307\n",
      "Epoch 156, loss: 781.936964\n",
      "Epoch 157, loss: 672.648108\n",
      "Epoch 158, loss: 661.639061\n",
      "Epoch 159, loss: 653.561224\n",
      "Epoch 160, loss: 877.493600\n",
      "Epoch 161, loss: 705.737065\n",
      "Epoch 162, loss: 799.587720\n",
      "Epoch 163, loss: 726.780336\n",
      "Epoch 164, loss: 727.447658\n",
      "Epoch 165, loss: 823.682755\n",
      "Epoch 166, loss: 700.952581\n",
      "Epoch 167, loss: 696.055100\n",
      "Epoch 168, loss: 784.085163\n",
      "Epoch 169, loss: 710.582210\n",
      "Epoch 170, loss: 685.854114\n",
      "Epoch 171, loss: 652.311161\n",
      "Epoch 172, loss: 746.922590\n",
      "Epoch 173, loss: 702.152056\n",
      "Epoch 174, loss: 778.274249\n",
      "Epoch 175, loss: 871.692043\n",
      "Epoch 176, loss: 646.498815\n",
      "Epoch 177, loss: 840.707049\n",
      "Epoch 178, loss: 728.415290\n",
      "Epoch 179, loss: 690.426840\n",
      "Epoch 180, loss: 739.665454\n",
      "Epoch 181, loss: 733.196557\n",
      "Epoch 182, loss: 689.120758\n",
      "Epoch 183, loss: 691.185690\n",
      "Epoch 184, loss: 690.395623\n",
      "Epoch 185, loss: 614.683168\n",
      "Epoch 186, loss: 779.076890\n",
      "Epoch 187, loss: 613.693324\n",
      "Epoch 188, loss: 737.198795\n",
      "Epoch 189, loss: 608.699022\n",
      "Epoch 190, loss: 948.151511\n",
      "Epoch 191, loss: 849.670219\n",
      "Epoch 192, loss: 687.716701\n",
      "Epoch 193, loss: 806.242125\n",
      "Epoch 194, loss: 770.989404\n",
      "Epoch 195, loss: 759.599821\n",
      "Epoch 196, loss: 804.542352\n",
      "Epoch 197, loss: 846.433513\n",
      "Epoch 198, loss: 684.157511\n",
      "Epoch 199, loss: 734.165473\n",
      "Epoch 0, loss: 717.204459\n",
      "Epoch 1, loss: 805.814468\n",
      "Epoch 2, loss: 725.602016\n",
      "Epoch 3, loss: 747.460138\n",
      "Epoch 4, loss: 935.970251\n",
      "Epoch 5, loss: 695.767047\n",
      "Epoch 6, loss: 1054.967770\n",
      "Epoch 7, loss: 855.559643\n",
      "Epoch 8, loss: 787.345628\n",
      "Epoch 9, loss: 717.795934\n",
      "Epoch 10, loss: 812.853297\n",
      "Epoch 11, loss: 728.364602\n",
      "Epoch 12, loss: 859.459328\n",
      "Epoch 13, loss: 924.298149\n",
      "Epoch 14, loss: 838.694169\n",
      "Epoch 15, loss: 742.095721\n",
      "Epoch 16, loss: 828.468344\n",
      "Epoch 17, loss: 736.243804\n",
      "Epoch 18, loss: 925.770390\n",
      "Epoch 19, loss: 784.370939\n",
      "Epoch 20, loss: 790.663032\n",
      "Epoch 21, loss: 889.655605\n",
      "Epoch 22, loss: 699.661615\n",
      "Epoch 23, loss: 841.165939\n",
      "Epoch 24, loss: 1108.895758\n",
      "Epoch 25, loss: 802.152605\n",
      "Epoch 26, loss: 774.447838\n",
      "Epoch 27, loss: 711.365324\n",
      "Epoch 28, loss: 782.993895\n",
      "Epoch 29, loss: 764.987277\n",
      "Epoch 30, loss: 703.205964\n",
      "Epoch 31, loss: 827.237579\n",
      "Epoch 32, loss: 1090.329369\n",
      "Epoch 33, loss: 996.522344\n",
      "Epoch 34, loss: 836.368452\n",
      "Epoch 35, loss: 717.392287\n",
      "Epoch 36, loss: 708.750811\n",
      "Epoch 37, loss: 883.323662\n",
      "Epoch 38, loss: 864.631529\n",
      "Epoch 39, loss: 711.622604\n",
      "Epoch 40, loss: 813.698463\n",
      "Epoch 41, loss: 698.436533\n",
      "Epoch 42, loss: 738.799520\n",
      "Epoch 43, loss: 925.150624\n",
      "Epoch 44, loss: 775.692954\n",
      "Epoch 45, loss: 778.497486\n",
      "Epoch 46, loss: 764.184503\n",
      "Epoch 47, loss: 728.538126\n",
      "Epoch 48, loss: 859.633950\n",
      "Epoch 49, loss: 692.702332\n",
      "Epoch 50, loss: 708.610999\n",
      "Epoch 51, loss: 758.120077\n",
      "Epoch 52, loss: 1085.327661\n",
      "Epoch 53, loss: 883.469388\n",
      "Epoch 54, loss: 757.573637\n",
      "Epoch 55, loss: 721.128182\n",
      "Epoch 56, loss: 892.631162\n",
      "Epoch 57, loss: 754.372124\n",
      "Epoch 58, loss: 654.463380\n",
      "Epoch 59, loss: 822.366289\n",
      "Epoch 60, loss: 878.808841\n",
      "Epoch 61, loss: 735.803634\n",
      "Epoch 62, loss: 670.453656\n",
      "Epoch 63, loss: 803.217370\n",
      "Epoch 64, loss: 740.077456\n",
      "Epoch 65, loss: 801.951814\n",
      "Epoch 66, loss: 866.296332\n",
      "Epoch 67, loss: 813.948927\n",
      "Epoch 68, loss: 746.898475\n",
      "Epoch 69, loss: 885.167498\n",
      "Epoch 70, loss: 784.132420\n",
      "Epoch 71, loss: 794.881614\n",
      "Epoch 72, loss: 766.644736\n",
      "Epoch 73, loss: 714.637212\n",
      "Epoch 74, loss: 688.625071\n",
      "Epoch 75, loss: 718.687377\n",
      "Epoch 76, loss: 726.561626\n",
      "Epoch 77, loss: 628.411822\n",
      "Epoch 78, loss: 715.653129\n",
      "Epoch 79, loss: 708.291582\n",
      "Epoch 80, loss: 737.667699\n",
      "Epoch 81, loss: 808.930544\n",
      "Epoch 82, loss: 870.606788\n",
      "Epoch 83, loss: 644.767172\n",
      "Epoch 84, loss: 815.095630\n",
      "Epoch 85, loss: 881.731581\n",
      "Epoch 86, loss: 725.764355\n",
      "Epoch 87, loss: 750.994433\n",
      "Epoch 88, loss: 778.008371\n",
      "Epoch 89, loss: 737.453593\n",
      "Epoch 90, loss: 681.992858\n",
      "Epoch 91, loss: 687.988925\n",
      "Epoch 92, loss: 864.845830\n",
      "Epoch 93, loss: 824.788258\n",
      "Epoch 94, loss: 673.624047\n",
      "Epoch 95, loss: 856.607751\n",
      "Epoch 96, loss: 801.398874\n",
      "Epoch 97, loss: 830.078113\n",
      "Epoch 98, loss: 794.278988\n",
      "Epoch 99, loss: 867.700280\n",
      "Epoch 100, loss: 725.612300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, loss: 797.392671\n",
      "Epoch 102, loss: 729.120402\n",
      "Epoch 103, loss: 873.149328\n",
      "Epoch 104, loss: 782.118748\n",
      "Epoch 105, loss: 704.311021\n",
      "Epoch 106, loss: 722.018203\n",
      "Epoch 107, loss: 754.805667\n",
      "Epoch 108, loss: 808.984852\n",
      "Epoch 109, loss: 670.492247\n",
      "Epoch 110, loss: 759.935147\n",
      "Epoch 111, loss: 707.699191\n",
      "Epoch 112, loss: 821.625338\n",
      "Epoch 113, loss: 738.105336\n",
      "Epoch 114, loss: 802.141422\n",
      "Epoch 115, loss: 799.517005\n",
      "Epoch 116, loss: 769.604635\n",
      "Epoch 117, loss: 689.911074\n",
      "Epoch 118, loss: 830.693655\n",
      "Epoch 119, loss: 838.373812\n",
      "Epoch 120, loss: 687.418299\n",
      "Epoch 121, loss: 673.661509\n",
      "Epoch 122, loss: 869.764102\n",
      "Epoch 123, loss: 818.806466\n",
      "Epoch 124, loss: 735.238293\n",
      "Epoch 125, loss: 748.532126\n",
      "Epoch 126, loss: 839.923389\n",
      "Epoch 127, loss: 821.274720\n",
      "Epoch 128, loss: 789.510489\n",
      "Epoch 129, loss: 771.290181\n",
      "Epoch 130, loss: 715.646355\n",
      "Epoch 131, loss: 678.720437\n",
      "Epoch 132, loss: 666.279049\n",
      "Epoch 133, loss: 911.214210\n",
      "Epoch 134, loss: 669.511245\n",
      "Epoch 135, loss: 947.116046\n",
      "Epoch 136, loss: 714.392134\n",
      "Epoch 137, loss: 771.997553\n",
      "Epoch 138, loss: 928.459112\n",
      "Epoch 139, loss: 651.458744\n",
      "Epoch 140, loss: 728.478369\n",
      "Epoch 141, loss: 764.203528\n",
      "Epoch 142, loss: 752.554089\n",
      "Epoch 143, loss: 679.108734\n",
      "Epoch 144, loss: 666.921947\n",
      "Epoch 145, loss: 745.818401\n",
      "Epoch 146, loss: 836.981954\n",
      "Epoch 147, loss: 819.850980\n",
      "Epoch 148, loss: 694.525681\n",
      "Epoch 149, loss: 766.575981\n",
      "Epoch 150, loss: 671.302611\n",
      "Epoch 151, loss: 809.147966\n",
      "Epoch 152, loss: 747.321421\n",
      "Epoch 153, loss: 700.883382\n",
      "Epoch 154, loss: 704.651273\n",
      "Epoch 155, loss: 636.254073\n",
      "Epoch 156, loss: 1031.289307\n",
      "Epoch 157, loss: 687.211936\n",
      "Epoch 158, loss: 791.163196\n",
      "Epoch 159, loss: 875.569932\n",
      "Epoch 160, loss: 999.139230\n",
      "Epoch 161, loss: 699.050760\n",
      "Epoch 162, loss: 784.664507\n",
      "Epoch 163, loss: 735.884187\n",
      "Epoch 164, loss: 823.892892\n",
      "Epoch 165, loss: 697.258068\n",
      "Epoch 166, loss: 673.445870\n",
      "Epoch 167, loss: 623.027538\n",
      "Epoch 168, loss: 806.769784\n",
      "Epoch 169, loss: 637.765110\n",
      "Epoch 170, loss: 906.054075\n",
      "Epoch 171, loss: 678.851227\n",
      "Epoch 172, loss: 678.458136\n",
      "Epoch 173, loss: 654.154791\n",
      "Epoch 174, loss: 798.776600\n",
      "Epoch 175, loss: 690.981287\n",
      "Epoch 176, loss: 825.397548\n",
      "Epoch 177, loss: 868.922927\n",
      "Epoch 178, loss: 767.248774\n",
      "Epoch 179, loss: 649.530574\n",
      "Epoch 180, loss: 773.249060\n",
      "Epoch 181, loss: 768.449358\n",
      "Epoch 182, loss: 648.625958\n",
      "Epoch 183, loss: 808.820922\n",
      "Epoch 184, loss: 904.793855\n",
      "Epoch 185, loss: 641.172141\n",
      "Epoch 186, loss: 821.343398\n",
      "Epoch 187, loss: 792.974792\n",
      "Epoch 188, loss: 781.693136\n",
      "Epoch 189, loss: 780.357493\n",
      "Epoch 190, loss: 805.981077\n",
      "Epoch 191, loss: 690.323766\n",
      "Epoch 192, loss: 895.723492\n",
      "Epoch 193, loss: 767.099781\n",
      "Epoch 194, loss: 694.348964\n",
      "Epoch 195, loss: 749.378159\n",
      "Epoch 196, loss: 687.426625\n",
      "Epoch 197, loss: 723.185019\n",
      "Epoch 198, loss: 755.241476\n",
      "Epoch 199, loss: 748.312424\n",
      "Epoch 0, loss: 949.919337\n",
      "Epoch 1, loss: 896.512972\n",
      "Epoch 2, loss: 845.353639\n",
      "Epoch 3, loss: 851.853537\n",
      "Epoch 4, loss: 801.284026\n",
      "Epoch 5, loss: 874.041779\n",
      "Epoch 6, loss: 809.239186\n",
      "Epoch 7, loss: 784.484167\n",
      "Epoch 8, loss: 766.308690\n",
      "Epoch 9, loss: 693.412841\n",
      "Epoch 10, loss: 763.655915\n",
      "Epoch 11, loss: 756.200144\n",
      "Epoch 12, loss: 821.979505\n",
      "Epoch 13, loss: 970.166425\n",
      "Epoch 14, loss: 900.585175\n",
      "Epoch 15, loss: 843.962613\n",
      "Epoch 16, loss: 842.241375\n",
      "Epoch 17, loss: 864.854019\n",
      "Epoch 18, loss: 898.769483\n",
      "Epoch 19, loss: 844.479466\n",
      "Epoch 20, loss: 974.805433\n",
      "Epoch 21, loss: 752.672936\n",
      "Epoch 22, loss: 655.758033\n",
      "Epoch 23, loss: 739.402524\n",
      "Epoch 24, loss: 663.376150\n",
      "Epoch 25, loss: 695.148709\n",
      "Epoch 26, loss: 816.192872\n",
      "Epoch 27, loss: 821.288810\n",
      "Epoch 28, loss: 837.432409\n",
      "Epoch 29, loss: 833.762869\n",
      "Epoch 30, loss: 708.328618\n",
      "Epoch 31, loss: 823.187126\n",
      "Epoch 32, loss: 1052.640723\n",
      "Epoch 33, loss: 761.327337\n",
      "Epoch 34, loss: 679.761816\n",
      "Epoch 35, loss: 752.493525\n",
      "Epoch 36, loss: 741.918956\n",
      "Epoch 37, loss: 753.204227\n",
      "Epoch 38, loss: 863.771060\n",
      "Epoch 39, loss: 705.498476\n",
      "Epoch 40, loss: 746.807733\n",
      "Epoch 41, loss: 756.640199\n",
      "Epoch 42, loss: 679.501021\n",
      "Epoch 43, loss: 697.378064\n",
      "Epoch 44, loss: 626.368326\n",
      "Epoch 45, loss: 750.640599\n",
      "Epoch 46, loss: 777.982102\n",
      "Epoch 47, loss: 747.557253\n",
      "Epoch 48, loss: 749.084827\n",
      "Epoch 49, loss: 867.711625\n",
      "Epoch 50, loss: 637.628895\n",
      "Epoch 51, loss: 734.153447\n",
      "Epoch 52, loss: 683.847308\n",
      "Epoch 53, loss: 727.349831\n",
      "Epoch 54, loss: 735.919315\n",
      "Epoch 55, loss: 707.308091\n",
      "Epoch 56, loss: 778.095569\n",
      "Epoch 57, loss: 671.188583\n",
      "Epoch 58, loss: 765.882243\n",
      "Epoch 59, loss: 828.579477\n",
      "Epoch 60, loss: 829.228022\n",
      "Epoch 61, loss: 789.121638\n",
      "Epoch 62, loss: 780.375533\n",
      "Epoch 63, loss: 771.383368\n",
      "Epoch 64, loss: 784.936928\n",
      "Epoch 65, loss: 859.618684\n",
      "Epoch 66, loss: 668.899931\n",
      "Epoch 67, loss: 900.175807\n",
      "Epoch 68, loss: 816.845630\n",
      "Epoch 69, loss: 733.076481\n",
      "Epoch 70, loss: 902.394507\n",
      "Epoch 71, loss: 676.094351\n",
      "Epoch 72, loss: 762.008931\n",
      "Epoch 73, loss: 791.371489\n",
      "Epoch 74, loss: 684.640262\n",
      "Epoch 75, loss: 761.372090\n",
      "Epoch 76, loss: 816.991186\n",
      "Epoch 77, loss: 679.159498\n",
      "Epoch 78, loss: 813.631546\n",
      "Epoch 79, loss: 751.561821\n",
      "Epoch 80, loss: 759.367438\n",
      "Epoch 81, loss: 770.331291\n",
      "Epoch 82, loss: 697.837763\n",
      "Epoch 83, loss: 765.653717\n",
      "Epoch 84, loss: 668.493454\n",
      "Epoch 85, loss: 691.028081\n",
      "Epoch 86, loss: 866.057327\n",
      "Epoch 87, loss: 809.017518\n",
      "Epoch 88, loss: 681.015262\n",
      "Epoch 89, loss: 741.345804\n",
      "Epoch 90, loss: 651.760494\n",
      "Epoch 91, loss: 654.341746\n",
      "Epoch 92, loss: 814.446269\n",
      "Epoch 93, loss: 688.546681\n",
      "Epoch 94, loss: 721.137381\n",
      "Epoch 95, loss: 750.396151\n",
      "Epoch 96, loss: 689.747858\n",
      "Epoch 97, loss: 742.171491\n",
      "Epoch 98, loss: 738.221170\n",
      "Epoch 99, loss: 650.235381\n",
      "Epoch 100, loss: 796.899332\n",
      "Epoch 101, loss: 746.884801\n",
      "Epoch 102, loss: 803.665877\n",
      "Epoch 103, loss: 913.140075\n",
      "Epoch 104, loss: 921.852889\n",
      "Epoch 105, loss: 643.936350\n",
      "Epoch 106, loss: 773.965472\n",
      "Epoch 107, loss: 741.184015\n",
      "Epoch 108, loss: 847.570052\n",
      "Epoch 109, loss: 686.003076\n",
      "Epoch 110, loss: 748.818523\n",
      "Epoch 111, loss: 804.591610\n",
      "Epoch 112, loss: 823.971836\n",
      "Epoch 113, loss: 810.307777\n",
      "Epoch 114, loss: 765.040205\n",
      "Epoch 115, loss: 714.932664\n",
      "Epoch 116, loss: 687.624864\n",
      "Epoch 117, loss: 654.774423\n",
      "Epoch 118, loss: 661.960727\n",
      "Epoch 119, loss: 1034.998516\n",
      "Epoch 120, loss: 747.409682\n",
      "Epoch 121, loss: 739.512152\n",
      "Epoch 122, loss: 680.846179\n",
      "Epoch 123, loss: 700.190947\n",
      "Epoch 124, loss: 693.734313\n",
      "Epoch 125, loss: 833.924832\n",
      "Epoch 126, loss: 628.851441\n",
      "Epoch 127, loss: 863.499773\n",
      "Epoch 128, loss: 711.816250\n",
      "Epoch 129, loss: 838.546413\n",
      "Epoch 130, loss: 799.292647\n",
      "Epoch 131, loss: 705.811982\n",
      "Epoch 132, loss: 695.667390\n",
      "Epoch 133, loss: 722.731749\n",
      "Epoch 134, loss: 681.151663\n",
      "Epoch 135, loss: 828.447372\n",
      "Epoch 136, loss: 868.830676\n",
      "Epoch 137, loss: 680.065405\n",
      "Epoch 138, loss: 819.646927\n",
      "Epoch 139, loss: 782.605245\n",
      "Epoch 140, loss: 686.856138\n",
      "Epoch 141, loss: 762.472097\n",
      "Epoch 142, loss: 760.176778\n",
      "Epoch 143, loss: 814.525285\n",
      "Epoch 144, loss: 665.777430\n",
      "Epoch 145, loss: 643.384943\n",
      "Epoch 146, loss: 655.738535\n",
      "Epoch 147, loss: 695.477095\n",
      "Epoch 148, loss: 674.275573\n",
      "Epoch 149, loss: 719.535311\n",
      "Epoch 150, loss: 692.933665\n",
      "Epoch 151, loss: 627.842391\n",
      "Epoch 152, loss: 674.118175\n",
      "Epoch 153, loss: 864.096760\n",
      "Epoch 154, loss: 900.541770\n",
      "Epoch 155, loss: 681.955277\n",
      "Epoch 156, loss: 750.175070\n",
      "Epoch 157, loss: 666.805042\n",
      "Epoch 158, loss: 641.694872\n",
      "Epoch 159, loss: 774.774357\n",
      "Epoch 160, loss: 762.260298\n",
      "Epoch 161, loss: 756.899468\n",
      "Epoch 162, loss: 761.145213\n",
      "Epoch 163, loss: 880.762527\n",
      "Epoch 164, loss: 604.015559\n",
      "Epoch 165, loss: 683.107649\n",
      "Epoch 166, loss: 789.462059\n",
      "Epoch 167, loss: 626.747028\n",
      "Epoch 168, loss: 671.382672\n",
      "Epoch 169, loss: 857.326543\n",
      "Epoch 170, loss: 652.812839\n",
      "Epoch 171, loss: 627.839691\n",
      "Epoch 172, loss: 786.494759\n",
      "Epoch 173, loss: 794.038367\n",
      "Epoch 174, loss: 729.057359\n",
      "Epoch 175, loss: 693.718411\n",
      "Epoch 176, loss: 665.835432\n",
      "Epoch 177, loss: 733.229117\n",
      "Epoch 178, loss: 707.769972\n",
      "Epoch 179, loss: 710.530853\n",
      "Epoch 180, loss: 676.934794\n",
      "Epoch 181, loss: 602.821191\n",
      "Epoch 182, loss: 721.202817\n",
      "Epoch 183, loss: 690.713011\n",
      "Epoch 184, loss: 633.662482\n",
      "Epoch 185, loss: 811.492963\n",
      "Epoch 186, loss: 668.647792\n",
      "Epoch 187, loss: 713.109622\n",
      "Epoch 188, loss: 771.322831\n",
      "Epoch 189, loss: 640.978692\n",
      "Epoch 190, loss: 668.463745\n",
      "Epoch 191, loss: 776.315261\n",
      "Epoch 192, loss: 730.063483\n",
      "Epoch 193, loss: 750.131892\n",
      "Epoch 194, loss: 756.883087\n",
      "Epoch 195, loss: 690.278147\n",
      "Epoch 196, loss: 665.825140\n",
      "Epoch 197, loss: 728.216865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198, loss: 838.788505\n",
      "Epoch 199, loss: 763.419467\n",
      "Epoch 0, loss: 684.258943\n",
      "Epoch 1, loss: 685.453136\n",
      "Epoch 2, loss: 677.239645\n",
      "Epoch 3, loss: 670.768140\n",
      "Epoch 4, loss: 664.030370\n",
      "Epoch 5, loss: 664.377179\n",
      "Epoch 6, loss: 658.945078\n",
      "Epoch 7, loss: 659.257088\n",
      "Epoch 8, loss: 652.653113\n",
      "Epoch 9, loss: 652.714485\n",
      "Epoch 10, loss: 656.368596\n",
      "Epoch 11, loss: 655.140881\n",
      "Epoch 12, loss: 647.264105\n",
      "Epoch 13, loss: 657.335447\n",
      "Epoch 14, loss: 655.196613\n",
      "Epoch 15, loss: 650.154499\n",
      "Epoch 16, loss: 656.914953\n",
      "Epoch 17, loss: 651.399671\n",
      "Epoch 18, loss: 660.137787\n",
      "Epoch 19, loss: 651.725606\n",
      "Epoch 20, loss: 646.631348\n",
      "Epoch 21, loss: 653.394793\n",
      "Epoch 22, loss: 633.171876\n",
      "Epoch 23, loss: 649.010230\n",
      "Epoch 24, loss: 637.136575\n",
      "Epoch 25, loss: 653.002001\n",
      "Epoch 26, loss: 657.008853\n",
      "Epoch 27, loss: 644.999751\n",
      "Epoch 28, loss: 644.625638\n",
      "Epoch 29, loss: 642.201534\n",
      "Epoch 30, loss: 637.633875\n",
      "Epoch 31, loss: 630.848811\n",
      "Epoch 32, loss: 630.464147\n",
      "Epoch 33, loss: 629.683814\n",
      "Epoch 34, loss: 629.894914\n",
      "Epoch 35, loss: 639.158992\n",
      "Epoch 36, loss: 632.767511\n",
      "Epoch 37, loss: 621.831472\n",
      "Epoch 38, loss: 656.743423\n",
      "Epoch 39, loss: 626.786538\n",
      "Epoch 40, loss: 627.202550\n",
      "Epoch 41, loss: 647.908856\n",
      "Epoch 42, loss: 630.434453\n",
      "Epoch 43, loss: 641.841562\n",
      "Epoch 44, loss: 643.172575\n",
      "Epoch 45, loss: 636.838126\n",
      "Epoch 46, loss: 643.410077\n",
      "Epoch 47, loss: 653.460130\n",
      "Epoch 48, loss: 634.583089\n",
      "Epoch 49, loss: 626.138128\n",
      "Epoch 50, loss: 624.888288\n",
      "Epoch 51, loss: 648.931933\n",
      "Epoch 52, loss: 635.504198\n",
      "Epoch 53, loss: 636.471689\n",
      "Epoch 54, loss: 633.906839\n",
      "Epoch 55, loss: 615.200741\n",
      "Epoch 56, loss: 639.268891\n",
      "Epoch 57, loss: 631.924287\n",
      "Epoch 58, loss: 632.265712\n",
      "Epoch 59, loss: 630.592212\n",
      "Epoch 60, loss: 638.616985\n",
      "Epoch 61, loss: 626.731088\n",
      "Epoch 62, loss: 627.947577\n",
      "Epoch 63, loss: 634.786489\n",
      "Epoch 64, loss: 648.600462\n",
      "Epoch 65, loss: 622.279732\n",
      "Epoch 66, loss: 630.275443\n",
      "Epoch 67, loss: 619.750052\n",
      "Epoch 68, loss: 626.560999\n",
      "Epoch 69, loss: 639.723806\n",
      "Epoch 70, loss: 633.795180\n",
      "Epoch 71, loss: 625.744373\n",
      "Epoch 72, loss: 623.228954\n",
      "Epoch 73, loss: 639.426089\n",
      "Epoch 74, loss: 626.141702\n",
      "Epoch 75, loss: 626.706739\n",
      "Epoch 76, loss: 657.842079\n",
      "Epoch 77, loss: 623.595953\n",
      "Epoch 78, loss: 615.024929\n",
      "Epoch 79, loss: 630.262371\n",
      "Epoch 80, loss: 620.087603\n",
      "Epoch 81, loss: 626.633075\n",
      "Epoch 82, loss: 622.965684\n",
      "Epoch 83, loss: 621.316825\n",
      "Epoch 84, loss: 610.157799\n",
      "Epoch 85, loss: 632.225762\n",
      "Epoch 86, loss: 653.697978\n",
      "Epoch 87, loss: 644.708330\n",
      "Epoch 88, loss: 637.640489\n",
      "Epoch 89, loss: 648.866562\n",
      "Epoch 90, loss: 627.510874\n",
      "Epoch 91, loss: 612.923693\n",
      "Epoch 92, loss: 630.761059\n",
      "Epoch 93, loss: 633.003120\n",
      "Epoch 94, loss: 621.997503\n",
      "Epoch 95, loss: 611.851142\n",
      "Epoch 96, loss: 622.616503\n",
      "Epoch 97, loss: 639.598000\n",
      "Epoch 98, loss: 617.992601\n",
      "Epoch 99, loss: 607.974525\n",
      "Epoch 100, loss: 618.547385\n",
      "Epoch 101, loss: 641.460509\n",
      "Epoch 102, loss: 620.447798\n",
      "Epoch 103, loss: 631.808396\n",
      "Epoch 104, loss: 628.947958\n",
      "Epoch 105, loss: 626.408989\n",
      "Epoch 106, loss: 622.886004\n",
      "Epoch 107, loss: 620.736635\n",
      "Epoch 108, loss: 638.692858\n",
      "Epoch 109, loss: 634.416432\n",
      "Epoch 110, loss: 606.810774\n",
      "Epoch 111, loss: 622.287782\n",
      "Epoch 112, loss: 623.237820\n",
      "Epoch 113, loss: 616.922830\n",
      "Epoch 114, loss: 633.992926\n",
      "Epoch 115, loss: 643.030833\n",
      "Epoch 116, loss: 632.788839\n",
      "Epoch 117, loss: 619.662632\n",
      "Epoch 118, loss: 635.741989\n",
      "Epoch 119, loss: 634.843402\n",
      "Epoch 120, loss: 624.560035\n",
      "Epoch 121, loss: 621.147493\n",
      "Epoch 122, loss: 628.650212\n",
      "Epoch 123, loss: 615.671878\n",
      "Epoch 124, loss: 635.754032\n",
      "Epoch 125, loss: 625.898542\n",
      "Epoch 126, loss: 622.614533\n",
      "Epoch 127, loss: 604.887021\n",
      "Epoch 128, loss: 637.777971\n",
      "Epoch 129, loss: 617.604617\n",
      "Epoch 130, loss: 620.365036\n",
      "Epoch 131, loss: 609.107236\n",
      "Epoch 132, loss: 628.567074\n",
      "Epoch 133, loss: 629.957195\n",
      "Epoch 134, loss: 600.817312\n",
      "Epoch 135, loss: 621.831692\n",
      "Epoch 136, loss: 624.375049\n",
      "Epoch 137, loss: 624.067323\n",
      "Epoch 138, loss: 630.991804\n",
      "Epoch 139, loss: 620.859980\n",
      "Epoch 140, loss: 623.413265\n",
      "Epoch 141, loss: 614.135639\n",
      "Epoch 142, loss: 607.045559\n",
      "Epoch 143, loss: 607.876547\n",
      "Epoch 144, loss: 626.898609\n",
      "Epoch 145, loss: 627.840108\n",
      "Epoch 146, loss: 607.887789\n",
      "Epoch 147, loss: 632.655590\n",
      "Epoch 148, loss: 634.219120\n",
      "Epoch 149, loss: 607.321106\n",
      "Epoch 150, loss: 615.866822\n",
      "Epoch 151, loss: 638.653303\n",
      "Epoch 152, loss: 624.797303\n",
      "Epoch 153, loss: 619.570981\n",
      "Epoch 154, loss: 624.657412\n",
      "Epoch 155, loss: 622.998872\n",
      "Epoch 156, loss: 638.247371\n",
      "Epoch 157, loss: 618.464927\n",
      "Epoch 158, loss: 609.900253\n",
      "Epoch 159, loss: 625.754094\n",
      "Epoch 160, loss: 608.558544\n",
      "Epoch 161, loss: 631.621066\n",
      "Epoch 162, loss: 622.937804\n",
      "Epoch 163, loss: 629.193875\n",
      "Epoch 164, loss: 637.686981\n",
      "Epoch 165, loss: 627.033062\n",
      "Epoch 166, loss: 616.043114\n",
      "Epoch 167, loss: 614.474470\n",
      "Epoch 168, loss: 634.266869\n",
      "Epoch 169, loss: 614.226641\n",
      "Epoch 170, loss: 623.374618\n",
      "Epoch 171, loss: 627.555344\n",
      "Epoch 172, loss: 644.235675\n",
      "Epoch 173, loss: 635.626372\n",
      "Epoch 174, loss: 634.159698\n",
      "Epoch 175, loss: 613.154905\n",
      "Epoch 176, loss: 616.642871\n",
      "Epoch 177, loss: 599.700140\n",
      "Epoch 178, loss: 608.808241\n",
      "Epoch 179, loss: 640.686295\n",
      "Epoch 180, loss: 628.058686\n",
      "Epoch 181, loss: 625.977075\n",
      "Epoch 182, loss: 628.346019\n",
      "Epoch 183, loss: 607.621012\n",
      "Epoch 184, loss: 613.502165\n",
      "Epoch 185, loss: 598.477322\n",
      "Epoch 186, loss: 636.626550\n",
      "Epoch 187, loss: 621.154872\n",
      "Epoch 188, loss: 613.322602\n",
      "Epoch 189, loss: 601.457409\n",
      "Epoch 190, loss: 604.776346\n",
      "Epoch 191, loss: 617.023695\n",
      "Epoch 192, loss: 607.452997\n",
      "Epoch 193, loss: 619.512405\n",
      "Epoch 194, loss: 608.782410\n",
      "Epoch 195, loss: 609.939893\n",
      "Epoch 196, loss: 619.847673\n",
      "Epoch 197, loss: 609.803668\n",
      "Epoch 198, loss: 625.489587\n",
      "Epoch 199, loss: 611.845655\n",
      "Epoch 0, loss: 688.504966\n",
      "Epoch 1, loss: 678.020864\n",
      "Epoch 2, loss: 680.061331\n",
      "Epoch 3, loss: 676.040237\n",
      "Epoch 4, loss: 665.221367\n",
      "Epoch 5, loss: 670.132968\n",
      "Epoch 6, loss: 662.182998\n",
      "Epoch 7, loss: 669.585884\n",
      "Epoch 8, loss: 653.164850\n",
      "Epoch 9, loss: 661.057646\n",
      "Epoch 10, loss: 657.711299\n",
      "Epoch 11, loss: 652.710469\n",
      "Epoch 12, loss: 655.809121\n",
      "Epoch 13, loss: 647.108819\n",
      "Epoch 14, loss: 653.737636\n",
      "Epoch 15, loss: 631.964768\n",
      "Epoch 16, loss: 647.592837\n",
      "Epoch 17, loss: 642.318250\n",
      "Epoch 18, loss: 639.954126\n",
      "Epoch 19, loss: 639.379149\n",
      "Epoch 20, loss: 660.886671\n",
      "Epoch 21, loss: 640.694491\n",
      "Epoch 22, loss: 640.245748\n",
      "Epoch 23, loss: 655.964721\n",
      "Epoch 24, loss: 632.388458\n",
      "Epoch 25, loss: 661.480249\n",
      "Epoch 26, loss: 612.625227\n",
      "Epoch 27, loss: 641.841068\n",
      "Epoch 28, loss: 641.374854\n",
      "Epoch 29, loss: 632.521026\n",
      "Epoch 30, loss: 647.044135\n",
      "Epoch 31, loss: 643.445774\n",
      "Epoch 32, loss: 631.810510\n",
      "Epoch 33, loss: 631.456047\n",
      "Epoch 34, loss: 644.097124\n",
      "Epoch 35, loss: 643.233050\n",
      "Epoch 36, loss: 624.761225\n",
      "Epoch 37, loss: 639.330457\n",
      "Epoch 38, loss: 640.766883\n",
      "Epoch 39, loss: 638.185082\n",
      "Epoch 40, loss: 634.961965\n",
      "Epoch 41, loss: 639.866317\n",
      "Epoch 42, loss: 644.459221\n",
      "Epoch 43, loss: 618.720743\n",
      "Epoch 44, loss: 631.627303\n",
      "Epoch 45, loss: 637.183480\n",
      "Epoch 46, loss: 643.431987\n",
      "Epoch 47, loss: 643.488765\n",
      "Epoch 48, loss: 621.901245\n",
      "Epoch 49, loss: 649.806518\n",
      "Epoch 50, loss: 636.960729\n",
      "Epoch 51, loss: 629.153287\n",
      "Epoch 52, loss: 631.137892\n",
      "Epoch 53, loss: 631.694967\n",
      "Epoch 54, loss: 639.004400\n",
      "Epoch 55, loss: 639.380453\n",
      "Epoch 56, loss: 631.105395\n",
      "Epoch 57, loss: 629.224255\n",
      "Epoch 58, loss: 619.543077\n",
      "Epoch 59, loss: 634.382853\n",
      "Epoch 60, loss: 634.694884\n",
      "Epoch 61, loss: 644.406668\n",
      "Epoch 62, loss: 613.300664\n",
      "Epoch 63, loss: 636.559472\n",
      "Epoch 64, loss: 619.439884\n",
      "Epoch 65, loss: 636.691080\n",
      "Epoch 66, loss: 635.885165\n",
      "Epoch 67, loss: 596.076273\n",
      "Epoch 68, loss: 628.906060\n",
      "Epoch 69, loss: 622.365216\n",
      "Epoch 70, loss: 632.820455\n",
      "Epoch 71, loss: 613.364092\n",
      "Epoch 72, loss: 627.568827\n",
      "Epoch 73, loss: 648.513584\n",
      "Epoch 74, loss: 648.553211\n",
      "Epoch 75, loss: 627.170579\n",
      "Epoch 76, loss: 623.310658\n",
      "Epoch 77, loss: 634.013144\n",
      "Epoch 78, loss: 629.165167\n",
      "Epoch 79, loss: 623.636314\n",
      "Epoch 80, loss: 629.950633\n",
      "Epoch 81, loss: 641.363196\n",
      "Epoch 82, loss: 626.423498\n",
      "Epoch 83, loss: 621.322909\n",
      "Epoch 84, loss: 616.681581\n",
      "Epoch 85, loss: 640.300287\n",
      "Epoch 86, loss: 642.579793\n",
      "Epoch 87, loss: 622.588470\n",
      "Epoch 88, loss: 647.322379\n",
      "Epoch 89, loss: 625.308651\n",
      "Epoch 90, loss: 627.325250\n",
      "Epoch 91, loss: 628.174218\n",
      "Epoch 92, loss: 622.031044\n",
      "Epoch 93, loss: 590.418624\n",
      "Epoch 94, loss: 636.823050\n",
      "Epoch 95, loss: 635.434797\n",
      "Epoch 96, loss: 611.630556\n",
      "Epoch 97, loss: 613.555873\n",
      "Epoch 98, loss: 629.368320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, loss: 628.736098\n",
      "Epoch 100, loss: 629.506393\n",
      "Epoch 101, loss: 622.735173\n",
      "Epoch 102, loss: 610.257245\n",
      "Epoch 103, loss: 642.660070\n",
      "Epoch 104, loss: 623.566180\n",
      "Epoch 105, loss: 633.321102\n",
      "Epoch 106, loss: 626.464711\n",
      "Epoch 107, loss: 643.919283\n",
      "Epoch 108, loss: 625.649524\n",
      "Epoch 109, loss: 627.502222\n",
      "Epoch 110, loss: 624.372869\n",
      "Epoch 111, loss: 628.115655\n",
      "Epoch 112, loss: 632.127192\n",
      "Epoch 113, loss: 606.665138\n",
      "Epoch 114, loss: 609.529419\n",
      "Epoch 115, loss: 619.754985\n",
      "Epoch 116, loss: 618.563960\n",
      "Epoch 117, loss: 620.618675\n",
      "Epoch 118, loss: 634.456290\n",
      "Epoch 119, loss: 629.118884\n",
      "Epoch 120, loss: 606.964139\n",
      "Epoch 121, loss: 622.750454\n",
      "Epoch 122, loss: 624.204039\n",
      "Epoch 123, loss: 622.101852\n",
      "Epoch 124, loss: 613.187386\n",
      "Epoch 125, loss: 629.433282\n",
      "Epoch 126, loss: 635.232373\n",
      "Epoch 127, loss: 624.450387\n",
      "Epoch 128, loss: 618.554148\n",
      "Epoch 129, loss: 627.308202\n",
      "Epoch 130, loss: 640.859661\n",
      "Epoch 131, loss: 623.855144\n",
      "Epoch 132, loss: 614.046460\n",
      "Epoch 133, loss: 617.703388\n",
      "Epoch 134, loss: 620.842038\n",
      "Epoch 135, loss: 631.032748\n",
      "Epoch 136, loss: 615.882415\n",
      "Epoch 137, loss: 620.522662\n",
      "Epoch 138, loss: 626.693340\n",
      "Epoch 139, loss: 620.495571\n",
      "Epoch 140, loss: 609.060501\n",
      "Epoch 141, loss: 605.864392\n",
      "Epoch 142, loss: 650.425784\n",
      "Epoch 143, loss: 627.819182\n",
      "Epoch 144, loss: 623.241910\n",
      "Epoch 145, loss: 638.208914\n",
      "Epoch 146, loss: 621.516663\n",
      "Epoch 147, loss: 631.277378\n",
      "Epoch 148, loss: 625.770533\n",
      "Epoch 149, loss: 620.981171\n",
      "Epoch 150, loss: 605.363416\n",
      "Epoch 151, loss: 619.716781\n",
      "Epoch 152, loss: 626.972673\n",
      "Epoch 153, loss: 607.265590\n",
      "Epoch 154, loss: 634.135507\n",
      "Epoch 155, loss: 614.765980\n",
      "Epoch 156, loss: 634.368129\n",
      "Epoch 157, loss: 617.512163\n",
      "Epoch 158, loss: 618.760997\n",
      "Epoch 159, loss: 628.841461\n",
      "Epoch 160, loss: 611.151135\n",
      "Epoch 161, loss: 626.981661\n",
      "Epoch 162, loss: 615.796084\n",
      "Epoch 163, loss: 615.808984\n",
      "Epoch 164, loss: 624.113807\n",
      "Epoch 165, loss: 622.168810\n",
      "Epoch 166, loss: 620.501266\n",
      "Epoch 167, loss: 617.297515\n",
      "Epoch 168, loss: 620.775818\n",
      "Epoch 169, loss: 623.419485\n",
      "Epoch 170, loss: 621.551741\n",
      "Epoch 171, loss: 615.305460\n",
      "Epoch 172, loss: 621.806941\n",
      "Epoch 173, loss: 596.050783\n",
      "Epoch 174, loss: 622.147792\n",
      "Epoch 175, loss: 623.935000\n",
      "Epoch 176, loss: 604.410384\n",
      "Epoch 177, loss: 627.882171\n",
      "Epoch 178, loss: 616.140901\n",
      "Epoch 179, loss: 606.226215\n",
      "Epoch 180, loss: 605.317883\n",
      "Epoch 181, loss: 623.357102\n",
      "Epoch 182, loss: 620.650498\n",
      "Epoch 183, loss: 634.121025\n",
      "Epoch 184, loss: 623.126873\n",
      "Epoch 185, loss: 631.192277\n",
      "Epoch 186, loss: 624.420521\n",
      "Epoch 187, loss: 598.563388\n",
      "Epoch 188, loss: 622.240626\n",
      "Epoch 189, loss: 606.666570\n",
      "Epoch 190, loss: 617.384507\n",
      "Epoch 191, loss: 624.107462\n",
      "Epoch 192, loss: 647.361539\n",
      "Epoch 193, loss: 620.774535\n",
      "Epoch 194, loss: 605.811321\n",
      "Epoch 195, loss: 604.604594\n",
      "Epoch 196, loss: 619.428974\n",
      "Epoch 197, loss: 635.760223\n",
      "Epoch 198, loss: 603.532929\n",
      "Epoch 199, loss: 610.093010\n",
      "Epoch 0, loss: 687.041987\n",
      "Epoch 1, loss: 676.775912\n",
      "Epoch 2, loss: 673.669625\n",
      "Epoch 3, loss: 669.858543\n",
      "Epoch 4, loss: 664.629424\n",
      "Epoch 5, loss: 662.531250\n",
      "Epoch 6, loss: 666.629801\n",
      "Epoch 7, loss: 653.794404\n",
      "Epoch 8, loss: 644.911919\n",
      "Epoch 9, loss: 650.510732\n",
      "Epoch 10, loss: 650.815393\n",
      "Epoch 11, loss: 657.197979\n",
      "Epoch 12, loss: 636.559576\n",
      "Epoch 13, loss: 650.433445\n",
      "Epoch 14, loss: 640.456935\n",
      "Epoch 15, loss: 641.867191\n",
      "Epoch 16, loss: 635.042917\n",
      "Epoch 17, loss: 642.732138\n",
      "Epoch 18, loss: 647.025542\n",
      "Epoch 19, loss: 644.433685\n",
      "Epoch 20, loss: 641.571810\n",
      "Epoch 21, loss: 633.526532\n",
      "Epoch 22, loss: 638.816675\n",
      "Epoch 23, loss: 651.818653\n",
      "Epoch 24, loss: 642.542842\n",
      "Epoch 25, loss: 624.443256\n",
      "Epoch 26, loss: 651.497890\n",
      "Epoch 27, loss: 630.131517\n",
      "Epoch 28, loss: 650.023356\n",
      "Epoch 29, loss: 636.301474\n",
      "Epoch 30, loss: 637.812059\n",
      "Epoch 31, loss: 652.500664\n",
      "Epoch 32, loss: 643.321903\n",
      "Epoch 33, loss: 616.885525\n",
      "Epoch 34, loss: 632.314417\n",
      "Epoch 35, loss: 631.761730\n",
      "Epoch 36, loss: 644.524198\n",
      "Epoch 37, loss: 649.965279\n",
      "Epoch 38, loss: 646.045719\n",
      "Epoch 39, loss: 643.586379\n",
      "Epoch 40, loss: 633.953503\n",
      "Epoch 41, loss: 630.317847\n",
      "Epoch 42, loss: 627.871535\n",
      "Epoch 43, loss: 617.594607\n",
      "Epoch 44, loss: 633.092502\n",
      "Epoch 45, loss: 620.461306\n",
      "Epoch 46, loss: 629.838601\n",
      "Epoch 47, loss: 636.533671\n",
      "Epoch 48, loss: 628.337468\n",
      "Epoch 49, loss: 624.171737\n",
      "Epoch 50, loss: 644.776775\n",
      "Epoch 51, loss: 629.409555\n",
      "Epoch 52, loss: 622.361501\n",
      "Epoch 53, loss: 639.392592\n",
      "Epoch 54, loss: 641.167411\n",
      "Epoch 55, loss: 629.305939\n",
      "Epoch 56, loss: 639.574118\n",
      "Epoch 57, loss: 622.199815\n",
      "Epoch 58, loss: 626.037459\n",
      "Epoch 59, loss: 623.932284\n",
      "Epoch 60, loss: 625.415469\n",
      "Epoch 61, loss: 634.591328\n",
      "Epoch 62, loss: 644.083852\n",
      "Epoch 63, loss: 627.863380\n",
      "Epoch 64, loss: 625.728753\n",
      "Epoch 65, loss: 647.881000\n",
      "Epoch 66, loss: 619.092245\n",
      "Epoch 67, loss: 620.033021\n",
      "Epoch 68, loss: 620.643944\n",
      "Epoch 69, loss: 634.554529\n",
      "Epoch 70, loss: 622.254071\n",
      "Epoch 71, loss: 624.096196\n",
      "Epoch 72, loss: 636.035689\n",
      "Epoch 73, loss: 621.823833\n",
      "Epoch 74, loss: 618.275457\n",
      "Epoch 75, loss: 624.184989\n",
      "Epoch 76, loss: 636.649117\n",
      "Epoch 77, loss: 610.639000\n",
      "Epoch 78, loss: 635.753961\n",
      "Epoch 79, loss: 621.948231\n",
      "Epoch 80, loss: 636.690414\n",
      "Epoch 81, loss: 613.752355\n",
      "Epoch 82, loss: 646.262052\n",
      "Epoch 83, loss: 626.039535\n",
      "Epoch 84, loss: 619.634000\n",
      "Epoch 85, loss: 639.967458\n",
      "Epoch 86, loss: 617.213371\n",
      "Epoch 87, loss: 608.047951\n",
      "Epoch 88, loss: 644.034295\n",
      "Epoch 89, loss: 638.286726\n",
      "Epoch 90, loss: 621.455748\n",
      "Epoch 91, loss: 620.553986\n",
      "Epoch 92, loss: 615.737391\n",
      "Epoch 93, loss: 636.963471\n",
      "Epoch 94, loss: 609.492055\n",
      "Epoch 95, loss: 633.749213\n",
      "Epoch 96, loss: 598.851822\n",
      "Epoch 97, loss: 636.552862\n",
      "Epoch 98, loss: 609.168321\n",
      "Epoch 99, loss: 616.112233\n",
      "Epoch 100, loss: 627.392204\n",
      "Epoch 101, loss: 627.196093\n",
      "Epoch 102, loss: 634.704602\n",
      "Epoch 103, loss: 636.385087\n",
      "Epoch 104, loss: 613.137558\n",
      "Epoch 105, loss: 627.605465\n",
      "Epoch 106, loss: 617.080573\n",
      "Epoch 107, loss: 615.844721\n",
      "Epoch 108, loss: 625.484663\n",
      "Epoch 109, loss: 613.797160\n",
      "Epoch 110, loss: 626.849804\n",
      "Epoch 111, loss: 634.151337\n",
      "Epoch 112, loss: 635.296726\n",
      "Epoch 113, loss: 626.264807\n",
      "Epoch 114, loss: 607.167859\n",
      "Epoch 115, loss: 635.665085\n",
      "Epoch 116, loss: 644.043801\n",
      "Epoch 117, loss: 625.578259\n",
      "Epoch 118, loss: 624.767521\n",
      "Epoch 119, loss: 621.061646\n",
      "Epoch 120, loss: 639.641670\n",
      "Epoch 121, loss: 617.808777\n",
      "Epoch 122, loss: 640.627361\n",
      "Epoch 123, loss: 608.444029\n",
      "Epoch 124, loss: 631.334119\n",
      "Epoch 125, loss: 619.075345\n",
      "Epoch 126, loss: 614.469843\n",
      "Epoch 127, loss: 630.175113\n",
      "Epoch 128, loss: 606.499775\n",
      "Epoch 129, loss: 598.326419\n",
      "Epoch 130, loss: 628.379341\n",
      "Epoch 131, loss: 626.215984\n",
      "Epoch 132, loss: 629.813065\n",
      "Epoch 133, loss: 611.509051\n",
      "Epoch 134, loss: 608.530487\n",
      "Epoch 135, loss: 611.034299\n",
      "Epoch 136, loss: 623.577644\n",
      "Epoch 137, loss: 603.318039\n",
      "Epoch 138, loss: 619.284891\n",
      "Epoch 139, loss: 613.056895\n",
      "Epoch 140, loss: 617.326306\n",
      "Epoch 141, loss: 619.107047\n",
      "Epoch 142, loss: 612.825669\n",
      "Epoch 143, loss: 618.502596\n",
      "Epoch 144, loss: 627.187055\n",
      "Epoch 145, loss: 646.030288\n",
      "Epoch 146, loss: 595.622912\n",
      "Epoch 147, loss: 623.045615\n",
      "Epoch 148, loss: 635.777330\n",
      "Epoch 149, loss: 636.846981\n",
      "Epoch 150, loss: 623.786993\n",
      "Epoch 151, loss: 620.439075\n",
      "Epoch 152, loss: 620.594085\n",
      "Epoch 153, loss: 619.398813\n",
      "Epoch 154, loss: 611.827645\n",
      "Epoch 155, loss: 631.181837\n",
      "Epoch 156, loss: 631.539449\n",
      "Epoch 157, loss: 627.409733\n",
      "Epoch 158, loss: 627.547778\n",
      "Epoch 159, loss: 625.383039\n",
      "Epoch 160, loss: 599.969187\n",
      "Epoch 161, loss: 630.671481\n",
      "Epoch 162, loss: 627.267333\n",
      "Epoch 163, loss: 635.485348\n",
      "Epoch 164, loss: 619.733046\n",
      "Epoch 165, loss: 619.692128\n",
      "Epoch 166, loss: 630.717498\n",
      "Epoch 167, loss: 618.199962\n",
      "Epoch 168, loss: 624.691253\n",
      "Epoch 169, loss: 615.345883\n",
      "Epoch 170, loss: 593.438106\n",
      "Epoch 171, loss: 625.115681\n",
      "Epoch 172, loss: 616.042292\n",
      "Epoch 173, loss: 623.410054\n",
      "Epoch 174, loss: 610.825408\n",
      "Epoch 175, loss: 593.022495\n",
      "Epoch 176, loss: 624.548194\n",
      "Epoch 177, loss: 614.420696\n",
      "Epoch 178, loss: 640.710198\n",
      "Epoch 179, loss: 607.017110\n",
      "Epoch 180, loss: 611.439728\n",
      "Epoch 181, loss: 626.099434\n",
      "Epoch 182, loss: 599.313491\n",
      "Epoch 183, loss: 622.090147\n",
      "Epoch 184, loss: 616.452591\n",
      "Epoch 185, loss: 623.571361\n",
      "Epoch 186, loss: 609.041510\n",
      "Epoch 187, loss: 632.597985\n",
      "Epoch 188, loss: 612.084101\n",
      "Epoch 189, loss: 624.270143\n",
      "Epoch 190, loss: 609.050201\n",
      "Epoch 191, loss: 621.688594\n",
      "Epoch 192, loss: 624.890850\n",
      "Epoch 193, loss: 605.761854\n",
      "Epoch 194, loss: 605.764509\n",
      "Epoch 195, loss: 626.437109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196, loss: 609.855409\n",
      "Epoch 197, loss: 619.070410\n",
      "Epoch 198, loss: 597.712600\n",
      "Epoch 199, loss: 626.100706\n",
      "Epoch 0, loss: 690.096380\n",
      "Epoch 1, loss: 689.087233\n",
      "Epoch 2, loss: 687.777467\n",
      "Epoch 3, loss: 687.649768\n",
      "Epoch 4, loss: 686.763387\n",
      "Epoch 5, loss: 686.305153\n",
      "Epoch 6, loss: 683.677895\n",
      "Epoch 7, loss: 684.525307\n",
      "Epoch 8, loss: 686.513056\n",
      "Epoch 9, loss: 684.589745\n",
      "Epoch 10, loss: 685.388493\n",
      "Epoch 11, loss: 680.437708\n",
      "Epoch 12, loss: 681.510793\n",
      "Epoch 13, loss: 681.645197\n",
      "Epoch 14, loss: 680.416105\n",
      "Epoch 15, loss: 682.275792\n",
      "Epoch 16, loss: 681.610271\n",
      "Epoch 17, loss: 679.206765\n",
      "Epoch 18, loss: 676.988646\n",
      "Epoch 19, loss: 677.160729\n",
      "Epoch 20, loss: 676.646496\n",
      "Epoch 21, loss: 678.916675\n",
      "Epoch 22, loss: 678.345553\n",
      "Epoch 23, loss: 675.475893\n",
      "Epoch 24, loss: 675.996525\n",
      "Epoch 25, loss: 675.168985\n",
      "Epoch 26, loss: 675.502784\n",
      "Epoch 27, loss: 675.038624\n",
      "Epoch 28, loss: 680.130328\n",
      "Epoch 29, loss: 672.598280\n",
      "Epoch 30, loss: 671.626142\n",
      "Epoch 31, loss: 672.558724\n",
      "Epoch 32, loss: 672.098640\n",
      "Epoch 33, loss: 670.607650\n",
      "Epoch 34, loss: 674.702975\n",
      "Epoch 35, loss: 666.573018\n",
      "Epoch 36, loss: 672.617086\n",
      "Epoch 37, loss: 669.443845\n",
      "Epoch 38, loss: 671.817957\n",
      "Epoch 39, loss: 667.732447\n",
      "Epoch 40, loss: 666.421600\n",
      "Epoch 41, loss: 671.661623\n",
      "Epoch 42, loss: 672.144401\n",
      "Epoch 43, loss: 672.390270\n",
      "Epoch 44, loss: 672.878718\n",
      "Epoch 45, loss: 669.661603\n",
      "Epoch 46, loss: 667.722378\n",
      "Epoch 47, loss: 663.278017\n",
      "Epoch 48, loss: 667.655852\n",
      "Epoch 49, loss: 661.656777\n",
      "Epoch 50, loss: 666.286958\n",
      "Epoch 51, loss: 664.514622\n",
      "Epoch 52, loss: 665.299645\n",
      "Epoch 53, loss: 667.000354\n",
      "Epoch 54, loss: 665.355299\n",
      "Epoch 55, loss: 666.501376\n",
      "Epoch 56, loss: 670.081238\n",
      "Epoch 57, loss: 652.607289\n",
      "Epoch 58, loss: 664.617999\n",
      "Epoch 59, loss: 659.412784\n",
      "Epoch 60, loss: 659.813939\n",
      "Epoch 61, loss: 659.275725\n",
      "Epoch 62, loss: 661.171564\n",
      "Epoch 63, loss: 665.588336\n",
      "Epoch 64, loss: 665.131865\n",
      "Epoch 65, loss: 668.291348\n",
      "Epoch 66, loss: 660.151467\n",
      "Epoch 67, loss: 651.868657\n",
      "Epoch 68, loss: 656.921421\n",
      "Epoch 69, loss: 654.752211\n",
      "Epoch 70, loss: 656.557504\n",
      "Epoch 71, loss: 660.284774\n",
      "Epoch 72, loss: 655.390567\n",
      "Epoch 73, loss: 662.669438\n",
      "Epoch 74, loss: 667.011225\n",
      "Epoch 75, loss: 657.191532\n",
      "Epoch 76, loss: 658.395172\n",
      "Epoch 77, loss: 664.983476\n",
      "Epoch 78, loss: 660.057671\n",
      "Epoch 79, loss: 656.177683\n",
      "Epoch 80, loss: 651.457403\n",
      "Epoch 81, loss: 651.049725\n",
      "Epoch 82, loss: 647.923730\n",
      "Epoch 83, loss: 657.293386\n",
      "Epoch 84, loss: 664.401305\n",
      "Epoch 85, loss: 656.865332\n",
      "Epoch 86, loss: 657.484530\n",
      "Epoch 87, loss: 662.579019\n",
      "Epoch 88, loss: 651.124593\n",
      "Epoch 89, loss: 649.211198\n",
      "Epoch 90, loss: 648.923996\n",
      "Epoch 91, loss: 647.987971\n",
      "Epoch 92, loss: 658.689585\n",
      "Epoch 93, loss: 650.167428\n",
      "Epoch 94, loss: 659.591452\n",
      "Epoch 95, loss: 667.971921\n",
      "Epoch 96, loss: 657.113322\n",
      "Epoch 97, loss: 645.332120\n",
      "Epoch 98, loss: 661.395944\n",
      "Epoch 99, loss: 657.363925\n",
      "Epoch 100, loss: 644.344290\n",
      "Epoch 101, loss: 657.531074\n",
      "Epoch 102, loss: 661.759207\n",
      "Epoch 103, loss: 652.646792\n",
      "Epoch 104, loss: 670.414591\n",
      "Epoch 105, loss: 648.493188\n",
      "Epoch 106, loss: 656.800126\n",
      "Epoch 107, loss: 654.616883\n",
      "Epoch 108, loss: 654.507660\n",
      "Epoch 109, loss: 653.099913\n",
      "Epoch 110, loss: 661.584148\n",
      "Epoch 111, loss: 641.100472\n",
      "Epoch 112, loss: 653.783494\n",
      "Epoch 113, loss: 659.266927\n",
      "Epoch 114, loss: 647.262168\n",
      "Epoch 115, loss: 658.527741\n",
      "Epoch 116, loss: 645.490795\n",
      "Epoch 117, loss: 655.223534\n",
      "Epoch 118, loss: 654.489494\n",
      "Epoch 119, loss: 654.534535\n",
      "Epoch 120, loss: 650.821345\n",
      "Epoch 121, loss: 645.036458\n",
      "Epoch 122, loss: 661.803220\n",
      "Epoch 123, loss: 636.498864\n",
      "Epoch 124, loss: 651.005654\n",
      "Epoch 125, loss: 652.544487\n",
      "Epoch 126, loss: 640.540787\n",
      "Epoch 127, loss: 649.034732\n",
      "Epoch 128, loss: 664.779454\n",
      "Epoch 129, loss: 668.804210\n",
      "Epoch 130, loss: 650.511919\n",
      "Epoch 131, loss: 647.078355\n",
      "Epoch 132, loss: 659.191677\n",
      "Epoch 133, loss: 638.560747\n",
      "Epoch 134, loss: 649.035956\n",
      "Epoch 135, loss: 645.200668\n",
      "Epoch 136, loss: 639.166024\n",
      "Epoch 137, loss: 648.556190\n",
      "Epoch 138, loss: 652.570943\n",
      "Epoch 139, loss: 651.431248\n",
      "Epoch 140, loss: 651.287538\n",
      "Epoch 141, loss: 633.651777\n",
      "Epoch 142, loss: 645.878122\n",
      "Epoch 143, loss: 646.526456\n",
      "Epoch 144, loss: 651.714621\n",
      "Epoch 145, loss: 642.823019\n",
      "Epoch 146, loss: 649.907629\n",
      "Epoch 147, loss: 628.615531\n",
      "Epoch 148, loss: 651.920819\n",
      "Epoch 149, loss: 644.117866\n",
      "Epoch 150, loss: 650.770092\n",
      "Epoch 151, loss: 645.009098\n",
      "Epoch 152, loss: 652.735309\n",
      "Epoch 153, loss: 649.798589\n",
      "Epoch 154, loss: 649.936846\n",
      "Epoch 155, loss: 645.325341\n",
      "Epoch 156, loss: 660.524199\n",
      "Epoch 157, loss: 646.058100\n",
      "Epoch 158, loss: 655.442018\n",
      "Epoch 159, loss: 653.529101\n",
      "Epoch 160, loss: 656.944744\n",
      "Epoch 161, loss: 640.502001\n",
      "Epoch 162, loss: 650.711284\n",
      "Epoch 163, loss: 642.163654\n",
      "Epoch 164, loss: 658.063430\n",
      "Epoch 165, loss: 651.030680\n",
      "Epoch 166, loss: 640.597888\n",
      "Epoch 167, loss: 653.480794\n",
      "Epoch 168, loss: 651.991823\n",
      "Epoch 169, loss: 648.468350\n",
      "Epoch 170, loss: 643.198067\n",
      "Epoch 171, loss: 642.235248\n",
      "Epoch 172, loss: 653.870466\n",
      "Epoch 173, loss: 640.290329\n",
      "Epoch 174, loss: 630.738740\n",
      "Epoch 175, loss: 638.095958\n",
      "Epoch 176, loss: 657.083863\n",
      "Epoch 177, loss: 646.609489\n",
      "Epoch 178, loss: 649.351169\n",
      "Epoch 179, loss: 646.363221\n",
      "Epoch 180, loss: 650.307947\n",
      "Epoch 181, loss: 657.689466\n",
      "Epoch 182, loss: 644.442674\n",
      "Epoch 183, loss: 650.372879\n",
      "Epoch 184, loss: 641.642252\n",
      "Epoch 185, loss: 649.202175\n",
      "Epoch 186, loss: 643.753277\n",
      "Epoch 187, loss: 659.243468\n",
      "Epoch 188, loss: 648.174211\n",
      "Epoch 189, loss: 655.752012\n",
      "Epoch 190, loss: 645.546451\n",
      "Epoch 191, loss: 633.939358\n",
      "Epoch 192, loss: 641.098199\n",
      "Epoch 193, loss: 645.420386\n",
      "Epoch 194, loss: 651.359183\n",
      "Epoch 195, loss: 657.959368\n",
      "Epoch 196, loss: 648.238026\n",
      "Epoch 197, loss: 640.230677\n",
      "Epoch 198, loss: 650.742349\n",
      "Epoch 199, loss: 633.376988\n",
      "Epoch 0, loss: 690.715726\n",
      "Epoch 1, loss: 689.008490\n",
      "Epoch 2, loss: 687.951678\n",
      "Epoch 3, loss: 688.082813\n",
      "Epoch 4, loss: 686.917371\n",
      "Epoch 5, loss: 687.140828\n",
      "Epoch 6, loss: 685.310841\n",
      "Epoch 7, loss: 685.596992\n",
      "Epoch 8, loss: 685.532355\n",
      "Epoch 9, loss: 681.462812\n",
      "Epoch 10, loss: 681.583118\n",
      "Epoch 11, loss: 682.991182\n",
      "Epoch 12, loss: 683.925809\n",
      "Epoch 13, loss: 681.937148\n",
      "Epoch 14, loss: 679.002937\n",
      "Epoch 15, loss: 682.228264\n",
      "Epoch 16, loss: 678.131685\n",
      "Epoch 17, loss: 680.979037\n",
      "Epoch 18, loss: 679.251020\n",
      "Epoch 19, loss: 677.381588\n",
      "Epoch 20, loss: 678.676241\n",
      "Epoch 21, loss: 674.806885\n",
      "Epoch 22, loss: 677.352289\n",
      "Epoch 23, loss: 673.161553\n",
      "Epoch 24, loss: 675.915016\n",
      "Epoch 25, loss: 674.489470\n",
      "Epoch 26, loss: 672.404319\n",
      "Epoch 27, loss: 674.513857\n",
      "Epoch 28, loss: 668.594336\n",
      "Epoch 29, loss: 673.126892\n",
      "Epoch 30, loss: 675.524609\n",
      "Epoch 31, loss: 671.248477\n",
      "Epoch 32, loss: 669.690387\n",
      "Epoch 33, loss: 670.709203\n",
      "Epoch 34, loss: 668.206897\n",
      "Epoch 35, loss: 668.888937\n",
      "Epoch 36, loss: 675.270114\n",
      "Epoch 37, loss: 666.887079\n",
      "Epoch 38, loss: 671.537002\n",
      "Epoch 39, loss: 664.464539\n",
      "Epoch 40, loss: 671.074385\n",
      "Epoch 41, loss: 670.076131\n",
      "Epoch 42, loss: 667.601879\n",
      "Epoch 43, loss: 661.574378\n",
      "Epoch 44, loss: 668.996457\n",
      "Epoch 45, loss: 667.108610\n",
      "Epoch 46, loss: 670.136061\n",
      "Epoch 47, loss: 665.503880\n",
      "Epoch 48, loss: 665.113569\n",
      "Epoch 49, loss: 661.251445\n",
      "Epoch 50, loss: 666.304601\n",
      "Epoch 51, loss: 670.299396\n",
      "Epoch 52, loss: 662.667077\n",
      "Epoch 53, loss: 668.709856\n",
      "Epoch 54, loss: 666.984995\n",
      "Epoch 55, loss: 664.835390\n",
      "Epoch 56, loss: 661.684889\n",
      "Epoch 57, loss: 668.719278\n",
      "Epoch 58, loss: 666.094166\n",
      "Epoch 59, loss: 659.854867\n",
      "Epoch 60, loss: 659.337588\n",
      "Epoch 61, loss: 660.953224\n",
      "Epoch 62, loss: 665.977139\n",
      "Epoch 63, loss: 668.744246\n",
      "Epoch 64, loss: 664.068297\n",
      "Epoch 65, loss: 655.657118\n",
      "Epoch 66, loss: 660.835765\n",
      "Epoch 67, loss: 663.050084\n",
      "Epoch 68, loss: 661.047792\n",
      "Epoch 69, loss: 650.702822\n",
      "Epoch 70, loss: 665.654583\n",
      "Epoch 71, loss: 652.780677\n",
      "Epoch 72, loss: 650.833377\n",
      "Epoch 73, loss: 658.150895\n",
      "Epoch 74, loss: 655.635871\n",
      "Epoch 75, loss: 662.816549\n",
      "Epoch 76, loss: 655.452869\n",
      "Epoch 77, loss: 657.138945\n",
      "Epoch 78, loss: 663.616037\n",
      "Epoch 79, loss: 658.681050\n",
      "Epoch 80, loss: 650.502600\n",
      "Epoch 81, loss: 656.825696\n",
      "Epoch 82, loss: 657.980232\n",
      "Epoch 83, loss: 662.397217\n",
      "Epoch 84, loss: 658.465427\n",
      "Epoch 85, loss: 658.943582\n",
      "Epoch 86, loss: 660.414850\n",
      "Epoch 87, loss: 647.088432\n",
      "Epoch 88, loss: 662.753322\n",
      "Epoch 89, loss: 654.820707\n",
      "Epoch 90, loss: 647.776259\n",
      "Epoch 91, loss: 659.301209\n",
      "Epoch 92, loss: 645.370648\n",
      "Epoch 93, loss: 651.862475\n",
      "Epoch 94, loss: 650.922570\n",
      "Epoch 95, loss: 651.046668\n",
      "Epoch 96, loss: 653.375975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, loss: 665.944109\n",
      "Epoch 98, loss: 652.178809\n",
      "Epoch 99, loss: 659.611168\n",
      "Epoch 100, loss: 663.517581\n",
      "Epoch 101, loss: 650.800526\n",
      "Epoch 102, loss: 654.397475\n",
      "Epoch 103, loss: 659.094103\n",
      "Epoch 104, loss: 648.867123\n",
      "Epoch 105, loss: 650.789006\n",
      "Epoch 106, loss: 637.826687\n",
      "Epoch 107, loss: 651.214504\n",
      "Epoch 108, loss: 653.246270\n",
      "Epoch 109, loss: 652.523971\n",
      "Epoch 110, loss: 650.379870\n",
      "Epoch 111, loss: 651.294788\n",
      "Epoch 112, loss: 655.529652\n",
      "Epoch 113, loss: 649.136705\n",
      "Epoch 114, loss: 658.752332\n",
      "Epoch 115, loss: 649.346853\n",
      "Epoch 116, loss: 652.028896\n",
      "Epoch 117, loss: 659.556837\n",
      "Epoch 118, loss: 654.715147\n",
      "Epoch 119, loss: 659.184322\n",
      "Epoch 120, loss: 652.528271\n",
      "Epoch 121, loss: 662.314652\n",
      "Epoch 122, loss: 644.727826\n",
      "Epoch 123, loss: 653.617799\n",
      "Epoch 124, loss: 651.186393\n",
      "Epoch 125, loss: 639.145557\n",
      "Epoch 126, loss: 644.738107\n",
      "Epoch 127, loss: 664.787426\n",
      "Epoch 128, loss: 652.501591\n",
      "Epoch 129, loss: 640.377082\n",
      "Epoch 130, loss: 644.026562\n",
      "Epoch 131, loss: 647.492423\n",
      "Epoch 132, loss: 664.198287\n",
      "Epoch 133, loss: 626.601302\n",
      "Epoch 134, loss: 653.309376\n",
      "Epoch 135, loss: 658.175404\n",
      "Epoch 136, loss: 647.464239\n",
      "Epoch 137, loss: 648.820307\n",
      "Epoch 138, loss: 646.481897\n",
      "Epoch 139, loss: 658.925662\n",
      "Epoch 140, loss: 646.160227\n",
      "Epoch 141, loss: 641.524621\n",
      "Epoch 142, loss: 644.436441\n",
      "Epoch 143, loss: 663.012411\n",
      "Epoch 144, loss: 646.376637\n",
      "Epoch 145, loss: 642.408805\n",
      "Epoch 146, loss: 659.861885\n",
      "Epoch 147, loss: 656.602086\n",
      "Epoch 148, loss: 645.241019\n",
      "Epoch 149, loss: 645.808198\n",
      "Epoch 150, loss: 651.751661\n",
      "Epoch 151, loss: 652.536250\n",
      "Epoch 152, loss: 642.200505\n",
      "Epoch 153, loss: 632.432275\n",
      "Epoch 154, loss: 637.018430\n",
      "Epoch 155, loss: 651.409349\n",
      "Epoch 156, loss: 643.922593\n",
      "Epoch 157, loss: 644.773203\n",
      "Epoch 158, loss: 639.864912\n",
      "Epoch 159, loss: 647.878436\n",
      "Epoch 160, loss: 641.284650\n",
      "Epoch 161, loss: 645.879684\n",
      "Epoch 162, loss: 648.826304\n",
      "Epoch 163, loss: 639.520830\n",
      "Epoch 164, loss: 644.125725\n",
      "Epoch 165, loss: 650.197126\n",
      "Epoch 166, loss: 645.207262\n",
      "Epoch 167, loss: 642.610888\n",
      "Epoch 168, loss: 656.197650\n",
      "Epoch 169, loss: 639.826929\n",
      "Epoch 170, loss: 645.830588\n",
      "Epoch 171, loss: 652.828177\n",
      "Epoch 172, loss: 640.756516\n",
      "Epoch 173, loss: 649.947231\n",
      "Epoch 174, loss: 654.417169\n",
      "Epoch 175, loss: 647.329657\n",
      "Epoch 176, loss: 640.058068\n",
      "Epoch 177, loss: 647.520602\n",
      "Epoch 178, loss: 649.059597\n",
      "Epoch 179, loss: 637.427080\n",
      "Epoch 180, loss: 642.917767\n",
      "Epoch 181, loss: 647.132147\n",
      "Epoch 182, loss: 644.088974\n",
      "Epoch 183, loss: 657.737287\n",
      "Epoch 184, loss: 642.313668\n",
      "Epoch 185, loss: 647.522599\n",
      "Epoch 186, loss: 636.066487\n",
      "Epoch 187, loss: 637.425031\n",
      "Epoch 188, loss: 633.128705\n",
      "Epoch 189, loss: 645.056977\n",
      "Epoch 190, loss: 643.329671\n",
      "Epoch 191, loss: 636.671495\n",
      "Epoch 192, loss: 648.561140\n",
      "Epoch 193, loss: 647.286975\n",
      "Epoch 194, loss: 643.330617\n",
      "Epoch 195, loss: 636.260599\n",
      "Epoch 196, loss: 641.373372\n",
      "Epoch 197, loss: 631.997625\n",
      "Epoch 198, loss: 653.217698\n",
      "Epoch 199, loss: 635.292127\n",
      "Epoch 0, loss: 690.552068\n",
      "Epoch 1, loss: 689.841094\n",
      "Epoch 2, loss: 688.569812\n",
      "Epoch 3, loss: 689.045665\n",
      "Epoch 4, loss: 687.146892\n",
      "Epoch 5, loss: 688.052037\n",
      "Epoch 6, loss: 685.837960\n",
      "Epoch 7, loss: 685.103308\n",
      "Epoch 8, loss: 682.327195\n",
      "Epoch 9, loss: 682.919614\n",
      "Epoch 10, loss: 682.348619\n",
      "Epoch 11, loss: 681.501765\n",
      "Epoch 12, loss: 678.622975\n",
      "Epoch 13, loss: 679.323389\n",
      "Epoch 14, loss: 682.022249\n",
      "Epoch 15, loss: 677.591406\n",
      "Epoch 16, loss: 678.060332\n",
      "Epoch 17, loss: 683.984169\n",
      "Epoch 18, loss: 676.546821\n",
      "Epoch 19, loss: 677.336005\n",
      "Epoch 20, loss: 680.408727\n",
      "Epoch 21, loss: 675.559774\n",
      "Epoch 22, loss: 675.667888\n",
      "Epoch 23, loss: 678.774238\n",
      "Epoch 24, loss: 674.256209\n",
      "Epoch 25, loss: 673.340675\n",
      "Epoch 26, loss: 678.233219\n",
      "Epoch 27, loss: 669.942562\n",
      "Epoch 28, loss: 671.634057\n",
      "Epoch 29, loss: 674.204308\n",
      "Epoch 30, loss: 669.106503\n",
      "Epoch 31, loss: 677.755397\n",
      "Epoch 32, loss: 671.958736\n",
      "Epoch 33, loss: 670.743916\n",
      "Epoch 34, loss: 669.792730\n",
      "Epoch 35, loss: 665.642096\n",
      "Epoch 36, loss: 670.715363\n",
      "Epoch 37, loss: 671.651020\n",
      "Epoch 38, loss: 671.806598\n",
      "Epoch 39, loss: 667.981060\n",
      "Epoch 40, loss: 673.363330\n",
      "Epoch 41, loss: 666.373747\n",
      "Epoch 42, loss: 664.448598\n",
      "Epoch 43, loss: 675.824042\n",
      "Epoch 44, loss: 664.387908\n",
      "Epoch 45, loss: 676.363754\n",
      "Epoch 46, loss: 669.889643\n",
      "Epoch 47, loss: 663.485149\n",
      "Epoch 48, loss: 667.475625\n",
      "Epoch 49, loss: 664.242296\n",
      "Epoch 50, loss: 661.637232\n",
      "Epoch 51, loss: 659.752952\n",
      "Epoch 52, loss: 664.610536\n",
      "Epoch 53, loss: 667.020277\n",
      "Epoch 54, loss: 666.961346\n",
      "Epoch 55, loss: 664.041985\n",
      "Epoch 56, loss: 664.250876\n",
      "Epoch 57, loss: 660.283103\n",
      "Epoch 58, loss: 663.518697\n",
      "Epoch 59, loss: 672.502290\n",
      "Epoch 60, loss: 659.141234\n",
      "Epoch 61, loss: 667.771485\n",
      "Epoch 62, loss: 656.448338\n",
      "Epoch 63, loss: 659.618425\n",
      "Epoch 64, loss: 658.721681\n",
      "Epoch 65, loss: 666.727146\n",
      "Epoch 66, loss: 651.291985\n",
      "Epoch 67, loss: 666.871355\n",
      "Epoch 68, loss: 660.686780\n",
      "Epoch 69, loss: 656.286292\n",
      "Epoch 70, loss: 667.545523\n",
      "Epoch 71, loss: 652.192009\n",
      "Epoch 72, loss: 663.239956\n",
      "Epoch 73, loss: 654.215327\n",
      "Epoch 74, loss: 662.868987\n",
      "Epoch 75, loss: 660.137916\n",
      "Epoch 76, loss: 653.590749\n",
      "Epoch 77, loss: 662.057745\n",
      "Epoch 78, loss: 669.967986\n",
      "Epoch 79, loss: 651.992306\n",
      "Epoch 80, loss: 662.031869\n",
      "Epoch 81, loss: 662.626090\n",
      "Epoch 82, loss: 659.088058\n",
      "Epoch 83, loss: 662.041850\n",
      "Epoch 84, loss: 670.676494\n",
      "Epoch 85, loss: 654.297140\n",
      "Epoch 86, loss: 651.540741\n",
      "Epoch 87, loss: 657.052386\n",
      "Epoch 88, loss: 653.584981\n",
      "Epoch 89, loss: 655.837321\n",
      "Epoch 90, loss: 649.083745\n",
      "Epoch 91, loss: 653.590927\n",
      "Epoch 92, loss: 666.513506\n",
      "Epoch 93, loss: 651.538687\n",
      "Epoch 94, loss: 655.876134\n",
      "Epoch 95, loss: 657.239626\n",
      "Epoch 96, loss: 656.006972\n",
      "Epoch 97, loss: 651.127965\n",
      "Epoch 98, loss: 660.049309\n",
      "Epoch 99, loss: 664.312148\n",
      "Epoch 100, loss: 653.953237\n",
      "Epoch 101, loss: 657.316087\n",
      "Epoch 102, loss: 649.818612\n",
      "Epoch 103, loss: 642.239136\n",
      "Epoch 104, loss: 657.046679\n",
      "Epoch 105, loss: 656.811201\n",
      "Epoch 106, loss: 661.131551\n",
      "Epoch 107, loss: 656.185348\n",
      "Epoch 108, loss: 650.458404\n",
      "Epoch 109, loss: 646.454442\n",
      "Epoch 110, loss: 639.724355\n",
      "Epoch 111, loss: 649.074038\n",
      "Epoch 112, loss: 654.131747\n",
      "Epoch 113, loss: 649.813690\n",
      "Epoch 114, loss: 644.015235\n",
      "Epoch 115, loss: 646.924781\n",
      "Epoch 116, loss: 657.828770\n",
      "Epoch 117, loss: 654.815776\n",
      "Epoch 118, loss: 656.992455\n",
      "Epoch 119, loss: 659.466746\n",
      "Epoch 120, loss: 654.781804\n",
      "Epoch 121, loss: 651.429077\n",
      "Epoch 122, loss: 650.614843\n",
      "Epoch 123, loss: 656.873944\n",
      "Epoch 124, loss: 645.136297\n",
      "Epoch 125, loss: 641.839995\n",
      "Epoch 126, loss: 655.345689\n",
      "Epoch 127, loss: 654.993654\n",
      "Epoch 128, loss: 654.857079\n",
      "Epoch 129, loss: 654.201396\n",
      "Epoch 130, loss: 657.132168\n",
      "Epoch 131, loss: 652.693679\n",
      "Epoch 132, loss: 650.175460\n",
      "Epoch 133, loss: 654.432098\n",
      "Epoch 134, loss: 646.116681\n",
      "Epoch 135, loss: 642.682347\n",
      "Epoch 136, loss: 649.656860\n",
      "Epoch 137, loss: 640.997920\n",
      "Epoch 138, loss: 656.357483\n",
      "Epoch 139, loss: 652.848031\n",
      "Epoch 140, loss: 637.570043\n",
      "Epoch 141, loss: 653.115046\n",
      "Epoch 142, loss: 656.158491\n",
      "Epoch 143, loss: 647.491508\n",
      "Epoch 144, loss: 656.618184\n",
      "Epoch 145, loss: 655.304188\n",
      "Epoch 146, loss: 647.143461\n",
      "Epoch 147, loss: 650.271199\n",
      "Epoch 148, loss: 642.449616\n",
      "Epoch 149, loss: 647.005549\n",
      "Epoch 150, loss: 646.394264\n",
      "Epoch 151, loss: 647.092756\n",
      "Epoch 152, loss: 641.942348\n",
      "Epoch 153, loss: 652.653491\n",
      "Epoch 154, loss: 662.683011\n",
      "Epoch 155, loss: 657.987760\n",
      "Epoch 156, loss: 650.787582\n",
      "Epoch 157, loss: 648.232237\n",
      "Epoch 158, loss: 648.637639\n",
      "Epoch 159, loss: 648.310391\n",
      "Epoch 160, loss: 642.690588\n",
      "Epoch 161, loss: 654.883401\n",
      "Epoch 162, loss: 666.777930\n",
      "Epoch 163, loss: 646.997314\n",
      "Epoch 164, loss: 648.298850\n",
      "Epoch 165, loss: 651.587573\n",
      "Epoch 166, loss: 641.384751\n",
      "Epoch 167, loss: 643.664089\n",
      "Epoch 168, loss: 651.319613\n",
      "Epoch 169, loss: 637.441784\n",
      "Epoch 170, loss: 649.092179\n",
      "Epoch 171, loss: 641.428889\n",
      "Epoch 172, loss: 655.129955\n",
      "Epoch 173, loss: 658.439037\n",
      "Epoch 174, loss: 647.984743\n",
      "Epoch 175, loss: 635.468193\n",
      "Epoch 176, loss: 654.660684\n",
      "Epoch 177, loss: 634.281937\n",
      "Epoch 178, loss: 639.908605\n",
      "Epoch 179, loss: 650.913642\n",
      "Epoch 180, loss: 649.065003\n",
      "Epoch 181, loss: 649.506770\n",
      "Epoch 182, loss: 641.853869\n",
      "Epoch 183, loss: 643.600152\n",
      "Epoch 184, loss: 637.935792\n",
      "Epoch 185, loss: 646.955609\n",
      "Epoch 186, loss: 647.165038\n",
      "Epoch 187, loss: 629.306819\n",
      "Epoch 188, loss: 641.331935\n",
      "Epoch 189, loss: 643.698570\n",
      "Epoch 190, loss: 655.206368\n",
      "Epoch 191, loss: 636.695899\n",
      "Epoch 192, loss: 646.674950\n",
      "Epoch 193, loss: 644.269703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, loss: 662.837467\n",
      "Epoch 195, loss: 651.572468\n",
      "Epoch 196, loss: 638.480666\n",
      "Epoch 197, loss: 652.064249\n",
      "Epoch 198, loss: 654.662086\n",
      "Epoch 199, loss: 642.813676\n",
      "best validation accuracy achieved: 0.216000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "#df = pd.DataFrame(columns = learning_rates, index = reg_strengths)\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for learning_rate in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        clf = linear_classifer.LinearSoftmaxClassifier()\n",
    "        clf.fit(train_X, train_y, batch_size = batch_size, learning_rate = learning_rate, reg = reg, epochs = num_epochs)\n",
    "        preds = clf.predict(test_X)\n",
    "        accuracy = multiclass_accuracy(preds, test_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = clf\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.216000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
